{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\"\n",
    "import json\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import argparse \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        \"pretrained/Qwen2.5-VL-3B-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        # attn_implementation=\"flash_attention_2\",\n",
    "        attn_implementation=\"eager\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "# default processor\n",
    "processor = AutoProcessor.from_pretrained(\"pretrained/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"demo_images/catdog.png\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": img_path,\n",
    "                \"resized_height\": 448,\n",
    "                \"resized_width\": 448, # 至少要(560 / 28) ** 2 = 400个token\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Find the dog in figure\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking = True, # 设置思考\n",
    ")\n",
    "\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# 添加可选参数，获取返回的attentions\n",
    "inputs['output_attentions'] = True\n",
    "for keys in inputs.keys():\n",
    "    print(keys)\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs,vit_attns = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_attns = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vit_attns))\n",
    "print(len(vl_attns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffaa32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_vit_attn = vit_attns[-1]\n",
    "print(last_vit_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_vl_attn = vl_attns[-1]\n",
    "print(last_vl_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef54bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs['input_ids']\n",
    "print(input_ids)\n",
    "print(input_ids.shape)\n",
    "print(\"\\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_token = 151655\n",
    "attn_token = 5562\n",
    "img_token_mask = (input_ids == image_token)\n",
    "img_token_positions = torch.nonzero(img_token_mask, as_tuple=True)[1]\n",
    "attn_idx = (input_ids == attn_token).nonzero(as_tuple=True)[1].item()\n",
    "print(img_token_positions)\n",
    "print(img_token_positions.shape)\n",
    "print(attn_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "def visualize_attention(img, attentions, attn_idx, img_token_idx, patch_size = 28):\n",
    "    \"\"\"img不带batch维度\"\"\"\n",
    "\n",
    "    # make the image divisible by the patch size\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - \\\n",
    "        img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "    w_featmap = img.shape[-2] // patch_size\n",
    "    h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "    nh = attentions.shape[1]  # number of head\n",
    "\n",
    "    # keep only the output patch attention\n",
    "    attentions = attentions[0, :, attn_idx, img_token_idx].reshape(nh, -1)\n",
    "\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(\n",
    "        0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().detach().numpy()\n",
    "\n",
    "    return attentions\n",
    "\n",
    "def plot_attention(img, attention, idx = -1):\n",
    "    n_heads = attention.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    text = [\"Original Image\", \"Head Mean\"]\n",
    "    for i, fig in enumerate([img, np.mean(attention, 0)]):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        print(fig.shape)\n",
    "        plt.imshow(fig, cmap='inferno',alpha=0.8)\n",
    "        plt.title(text[i])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n_heads):\n",
    "        plt.subplot((n_heads + 2)//3, 3, i+1)\n",
    "        plt.imshow(attention[i], cmap='inferno')\n",
    "        plt.title(f\"Head n: {i+1}\")\n",
    "    plt.tight_layout()\n",
    "    if idx != -1:\n",
    "        \n",
    "        plt.savefig(f\"temp/demo_{idx}.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img,(448,448))\n",
    "img = torch.tensor(img).permute(2,0,1)\n",
    "plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_attens = visualize_attention(img,last_vl_attn.to(torch.float32),attn_idx,img_token_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29179dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(img.permute(1,2,0),show_attens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb682d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_each_layers(vl_attns):\n",
    "    for i in range(len(vl_attns)):\n",
    "        show_attens = visualize_attention(img,vl_attns[i].to(torch.float32),attn_idx,img_token_positions)\n",
    "        plot_attention(img.permute(1,2,0),show_attens,idx = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c068b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "show_each_layers(vl_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(img, attention, idx = -1):\n",
    "    n_heads = attention.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    text = [\"Original Image\", \"Head Mean\"]\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(np.mean(attention, 0), cmap='inferno', alpha=0.5)\n",
    "    plt.title(text[1])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n_heads):\n",
    "        plt.subplot((n_heads + 2)//3, 3, i+1)\n",
    "        plt.imshow(attention[i], cmap='inferno')\n",
    "        plt.title(f\"Head n: {i+1}\")\n",
    "    plt.tight_layout()\n",
    "    if idx != -1:\n",
    "        \n",
    "        plt.savefig(f\"temp/demo_{idx}.jpg\")\n",
    "    plt.show()\n",
    "plot_attention(img.permute(1,2,0), show_attens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0617204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
